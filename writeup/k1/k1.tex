\documentclass{article}
\usepackage{listings}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}

\begin{document}
\section{Instructions}
\input{|"cat ../../README.md | sed -e '1,/BEGIN LaTeX INCLUDED/d' | pandoc -f markdown -t latex"}
\section{Design Decisions}
\subsection{Priority Queue}
The priority queue used to implement the ready queue of the scheduler is
arguably the most complex data structure used in the kernel.
The data structure currently being used is a series of FIFO queues, one for
each priority level.
When a task is enqueued, it is added to the queue corresponding to its priority
level.
When a consuming a task from the priority queue, a task is dequeued from the
non-empty queue with the highest priority.
Each individual queue is implemented as a doubly-linked list, where the
\texttt{next} and \texttt{prev} pointers of each node are part of the task
descriptor itself.
Doing this is possible since a task is in at most one queue at any given time.
Inlining these fields allows us to not need to separately allocate memory for
linked list nodes, which simplifies the design.
For efficiency, we maintain a bit-vector which allows us to efficiently find
the non-empty queue with the highest priority.
The $i$th bit of the bit vector is set if the queue of priority $i$ is non-empty.
To find the highest priority non-empty queue, can find the index of the least
significant set bit, which can be done fairly efficiently.

This implementation is reasonably efficient in both memory and time.
The memory usage is linear in the number of task descriptors, no matter
how many queues need to be created.
Moreover, the time taken to enqueue and dequeue task descriptors is constant,
and fairly fast in practice.

To be fair, this implementation is almost exactly what was suggested in the lecture.
This is by no means novel work.

We did consider alternate implementations at various points, and eventually rejected them.
The first implementation done was a priority queue implemented as a fixed-capacity heap.
However, enqueueing and dequeueing elements from the heap takes $O(\log(n))$ time,
which is too slow for our purposes.

Another implementation considered was a ringbuffer storing pointers to task descriptors.
However, we then need to have multiple ringbuffers, one for each priority level.
Each buffer needs to have sufficient space allocated to accommodate the maximum
number of tasks.
Therefore, this implementation consumed $O(kn)$ memory, where $k$ is the number
of priority levels, and $n$ is the number of tasks.
This would have gotten worse yet after implementing message passing, where each
a queue is needed for each task.
At this point, memory consumption would be $O(n^2)$, which is simply not scalable.

\subsection{Integer Log}
As a subproblem of implementing the priority queue, we needed to be able to take
a bit vector $b$, and quickly find the least significant set bit.
This allows us to quickly determine the right constituent queue to dequeue the next
task from.
To do this, we first calculate \texttt{c = b \& -b}, which leaves only the least significant
bit of $b$ set.

After that point, we want to determine the index of the set bit, which is the
base-2 log of the number.
The method used is inspired by John Owens' method as listed on the Stanford bit
twiddling hacks page, but has been altered in implementation to take fewer
instructions on the ARM processor.
The algorithm essentially binary searches through the bit vector for the set bit.

At each step, we know that the set bit is in the bottom $2i$ bits of $c$, and we want
to narrow that down to a range of $i$ bits.
At each step, we run the following pseudo-code:

\begin{center}
\begin{lstlisting}
if (c >> i) {
    c = c >> i;
    log += i;
}
\end{lstlisting}
\end{center}

If the set bit is in the high $i$ bits, then we divide by $2^i$, and add $i$ to the log.
If the set bit is in the low $i$ bits, we do nothing.
We then iterate to find the log of the new (or possibly unchanged) value of $c$.
At the end of this step, we know that set bit is within the bottom $i$ bits of $c$.
We perform this process for $i=16$, 8 and 4.

A previous copy of the algorithm then repeated the process for a shift of 2 bits and 1 bit
to find the final result.
However, it turns out that there is a trick that can be used for the last 4 cases.

Once $2i=4$, 4 values of $c$ are possible: 8, 4, 2 and 1.
The integer logs of these are 3, 2, 1 and 0 respectively.
It turns out that this is equal to \texttt{c / 2 - c / 8},
assuming integer division.
Since these divisions can be translated into bit-shifts, and because
ARM allows the second operand to many instructions to be bit-shifted essentially
for free, it turns out that this is a very cheap expression to calculate.
This trick can be used to step the last two iterations (each of which costs 3
instructions), and replace it with an addition and subtraction operation,
thereby whittling 4 instruction off the total.

The entire algorithm takes 14 instructions of hand-written assembly.

We consider the result to be undefined if $b=0$, and expect the caller to check
this case before running this algorithm to try to find the least significant set bit.
The $b=0$ case is special to the caller anyway, and needs to be handled separately,
and doing this makes it easier to optimize the algorithm.

\subsection{Task Resource Allocation}
All task descriptors are simply allocated in a statically allocated table.
A task ID is just an integer index into that table.
This approach makes the implementation very simple, but precludes the reuse
of memory allocated for task descriptors, since task IDs themselves cannot
be reused.
If we should ever want to support reusing task resources after a task exits,
we will need to switch the task descriptor table to a different implementation.

Memory allocated for task stacks is allocated in 4K blocks after the end of
the kernel's stack.
Each individual stack grows down, but the overall set of task stacks grows up in
4K increments.
Like the task descriptors themselves, memory is not currently reused after the
task exits.

\subsection{Argument Passing}
To pass arguments and return values to and from syscalls, the kernel manipulates
the context of the task as stored in memory.
During the context switch, all the registers of the user task are stored on the
user's stack, including \texttt{r0}-\texttt{r3}, which are used to pass arguments.
To find the arguments passed to the syscall, the kernel can just read the values
from the saved context.
To return a value to the user task, the kernel can write to the stored value of
\texttt{r0}, which will be restored to the user task when context switching back,
and will be interpreted as a return argument.

This approach has two advantages.
Firstly, returning values this way makes it very easy to handle returning values
to tasks which aren't immediately returned to.
The kernel just stores the return value in the context, and doesn't need to care
when that context is restored.
Secondly, it makes the context switching code very simple, since no special treatment
of the arguments is required.
The context switching simply preserves the entire context, and is agnostic to what,
if any, arguments have been passed to the syscall.

However, this approach does result in unnecessary memory accesses.
The obvious way to avoid this would be to pass the syscall arguments back to the kernel
in their original registers.
As it turns out, trying to make a C function have returns in registers (i.e.:\ a 4-member
struct returned in \texttt{r0}-\texttt{r3}) is not easy to do.
Some compilers implement non-standard extensions which support doing this, but GCC
is not one of them.
There is a plausible workaround which uses explicit register variables.
The function would return \texttt{void}, but then return values could be picked up
by accessing variables declared as being stored in \texttt{r0}-\texttt{r3}.
This would allow us to not save those registers in memory for syscalls, since
there is no expectation that those values will be preserved through the function call.
They just need to be passed to the kernel, not restored when the context switches back.
We only need to preserve the full context during interrupts, which is
currently out of scope.
However, I've had considerable difficulty in my past experiences with trying to
convince GCC to emit the correct assembly code to not clobber values returned in this
way.
Therefore, I'm not going to try to implement this solution unless it becomes a
performance problem.

\section{Source Code}
The source code is hosted on git, the most recent revision is
\input{|"git rev-parse HEAD | ./verbatim"}
We are submitting the following files:
\input{|"cat file-list | ./verbatim"}
\input{|"./file-list | ./verbatim"}
\section{Questions}
A program with the user processes as specificed in the manual outputs:
\begin{verbatim}
Boot...IO...
e1:0
e2:0
Created: 1
Created: 2
Child (tid=3, parent_tid=0)
Child (tid=3, parent_tid=0)
Created: 3
Child (tid=4, parent_tid=0)
Child (tid=4, parent_tid=0)
Created: 4
First: exiting
Child (tid=1, parent_tid=0)
Child (tid=2, parent_tid=0)
Child (tid=1, parent_tid=0)
Child (tid=2, parent_tid=0)
\end{verbatim}
\begin{itemize}
\item The first 3 lines are printed as the OS boots up, and are printed the
	same, regardless of the user task configuration.
\item The next two "Created:" lines are printed before their corresponding
	tasks get a chance to run, because the tasks which they correspond to
	are lower priority than the init task which created them.
\item The next two "Child" lines are printed before their corresponding
	"Created" lines, because their corresponding tasks are higher priority
	than the init task which created them, and hence get scheduled first.
\item The init task exits before the first two tasks get a chance to run,
	because both of them are lower priority than than init task itself, and
	hence will never get scheduled while the init task is alive (they cannot
	get blocked in this version of the kernel).
\item Once the first two tasks are finally scheduled, their output is
	interwoven because our scheduling is round-robin between tasks of the
	same priority.
\end{itemize}

\end{document}
