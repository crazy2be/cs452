\documentclass[titlepage]{article}
\usepackage{listings}
\usepackage{hyperref}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}

\begin{document}
\title{Kernel 2}
\author{Justin McGirr (\#20413625), Peter Raboud (\#20437716)}
\maketitle

\section{Instructions}
\input{"../k1/README.tex"}
\section{Design Decisions}
Very little has changed with the architecture of the kernel since the previous assignment.

One change was to actually store the current state of a task in it's task descriptor.
Previously, the state of the task could be inferred, and did not need to be explicitly stored.
(If a task was currently being executed, it is active; if in the ready queue, it is ready; otherwise
it is a zombie. No other possible states existed.)
We now need to store whether a task is in the \texttt{REPLY\_BLK} or \texttt{RECV\_BLK} states,
since this cannot be inferred by whether the task is on some particular queue.

We also added a send queue attached to each task descriptor.
This is a FIFO queue which maintains a list of task descriptors which wish to send a message
to that task.
The FIFO queue was actually already implemented as a subproblem of the priority queue used
for the ready queue, and so very little new code needed to be written for this.

\section{Timings}
All of these timings were generated by running the code in question 200 times,
and taking the average, in order to give the caches enough time to warm up, and
to avoid timing jitter.
We also reset the hardware between each trial run.

\input{|"python timings.py latex"}

The fact that there is a significant difference between the time taken for 4
byte and 64 byte messages suggests that a significant amount of time is spent
just copying messages back and forth, at least when the messages are of a
nontrivial size. It's possible that our very naive \texttt{memcpy} implementation could
be significantly speed up by forcing memory buffers passed in from user tasks
to be word aligned, and then doing a word-by-word copy, rather than a byte-by-
byte copy.

Another surprising thing is the magnitude of the difference with and without
optimizations- a factor of nearly 3x, persistent across all the different
incarnations. This suggests that a significant portion of our time is actually
spent in C kernel code, not in hand-written assembly such as the context switch,
which I would have initially expected to take quite a bit of time, given that
there is not very much C code in the kernel as it exists right now.

Sending before replying made almost no difference in runtime.
The only difference caused by the send/receive order is whether the sender is
enqueued on the receiver's send queue or not.
That there is little performance difference implies that our queue
implementation is quite fast, and doesn't take a large proportion of the overall
runtime (~7us difference).

Caches made a significant difference, almost 2x, implying that the processor was
spending quite a quite a bit of time spinning idly, waiting for the memory to
get back to it. Curiously, it made less of a difference with optimizations
turned on, potentially because the code generated is smaller in this case (but
takes longer to run).
A clever compiler could generate code to avoid memory access and therefore
avoid data stalls.
The observation that caching makes less of a difference for the optimized program
supports this hypothesis.

\section{Questions}
We chose to give both the client and server tasks priorities lower than
the first user task.
This ensures that the first user task starts the server and all clients
before anything else begins running, which makes the rest of the execution
easier to reason about.
This is, however, not strictly necessary for the correct running of the program.

We give the same priority to both the server and clients.
The only condition that we need to ensure the correct operation of the program
is that the server registers with the nameserver before
any of the clients attempt to look up the TID of the server.
Since the server task is created first, and tasks of the same priority are scheduled in FIFO order,
we know that the server will execute first.
Since the nameserver processes requests in FIFO order, this is sufficient to
ensure that the server will register first.

The output produced by the rock-paper-scissors programs is as follows:

\begin{verbatim}
Boot...IO...
e1:0
e2:0
Client 4 connected
Client 3 connected
Client 6 connected
Client 5 connected
Client 3 drew round 0 with rock
Client 4 drew round 0 with rock
Client 5 won round 0 with paper
Client 6 lost round 0 with rock
Client 3 won round 1 by default, now quitting...
Client 4 quit
Client 6 drew round 1 with scissors
Client 5 drew round 1 with scissors
Client 5 lost round 2 with paper
Client 6 won round 2 with scissors
Client 5 won round 3 by default, now quitting...
RPS server done
Client 6 quit
\end{verbatim}

We shall now explain each part of the output.

\begin{verbatim}
Boot...IO...
e1:0
e2:0
\end{verbatim}

The first 3 lines are produced as part of the boot-up sequence of the kernel.

\begin{verbatim}
Client 4 connected
Client 3 connected
Client 6 connected
Client 5 connected
\end{verbatim}

The next 4 connected lines are produced by the client after it connects for
the first time.
Each client uses its TID as its identifying number.
The lines are produced in pairs, as two clients are paired.
Since the clients begin executing in the order they were created, the lines are
overall ordered in ascending order of TID\@.
However, since the server replies to the second client of the pair first,
the order is reversed within each pair of lines.

\begin{verbatim}
Client 3 drew round 0 with rock
Client 4 drew round 0 with rock
Client 5 won round 0 with paper
Client 6 lost round 0 with rock
\end{verbatim}

Subsequent rounds are played by each client selecting a move by asking the kernel
for a pseudo-random number (generated by the Mersenne twister algorithm).
Each client decides on a random number of rounds to play at the start, which may not
be the same as the number its partner decides.
Subsequent lines each represent a pair of clients reporting the result of a round.
The order in which the clients report reverses each round, since the server replies to
the last client first.
Since the client which was last sends the message for the next round before it partner,
the roles reverse each round.

\begin{verbatim}
Client 3 won round 1 by default, now quitting...
Client 4 quit
\end{verbatim}

We then see that client 4 quits after only 1 round.
In round 2, it would have sent the quit response to the server after its partner, client 3,
sent its move to the server.
When the server responded to the quit request, replying to client 4, it first notified client 3
that it's partner quit.
We therefore see client 3 report that it won by default, and then exits.
Then, the server responds to client 4, which reports that it has quit, then exits.

\begin{verbatim}
Client 6 drew round 1 with scissors
Client 5 drew round 1 with scissors
Client 5 lost round 2 with paper
Client 6 won round 2 with scissors
Client 5 won round 3 by default, now quitting...
RPS server done
Client 6 quit
\end{verbatim}

These lines follow much the same pattern as explained above
We do see that because the RPS server
The one thing of interest is that the RPS server quits before client 6 quits.
This is because the server sends a quit message to client 5, then to client 6.
The kernel schedules the sending task before the replying task after a reply.
However, client 6 calls \texttt{tid}, which yields control to the server,
and allows it to print and exit first.

\section{Source Code}
The source code is hosted on git, at \url{git.uwaterloo.ca/pgraboud/cs452-kernel}.
The version we wish to submit is on the \texttt{k2} branch, specifically
the commit:
\input{|"git rev-parse k2 | ./../verbatim"}
We are submitting the following files:
\input{|"cat ../file-list | ./../verbatim"}
\input{|"./../file-list | ./../verbatim"}

\end{document}
